{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SC_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waynchi/SC-Net/blob/master/SC_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U22qZkbsfEyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mnist\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45bLorRNFfdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LsQqrsgeqA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.is_gpu_available()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abqHOFhAxDBI",
        "colab_type": "text"
      },
      "source": [
        "# Experiment Notes\n",
        "\n",
        "- 1 vs 2.5 upper bound (1 seems to work fine)\n",
        "- Deeper vs more filters\n",
        "  - 8 as lowest with 32 filters ( ~750k parameters) gave blurry results around 900 epochs for overfitting test of 2. Also, seems to have overfit on only one example.\n",
        "  - 4 as lowest with 32 filters ( ~2.9M parameters) gave blurry results for overfitting test of 2 around 1000 epochs. \n",
        "  - 8 as lowest with 64 filters (~2.9M parameters) gave...\n",
        "  - 4 as lowest with 64 filters (~11M parameters) gave.... Way to slow. Also accuracy was still only ~0.05 after 800 epochs on 100 samples\n",
        "- The biggest issue is that the intensity accuracy stays around 0.05 which is far too low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeRjiZ_-Du5O",
        "colab_type": "text"
      },
      "source": [
        "Debugging Steps\n",
        "\n",
        "1. Does it work for pure orderless NADE?\n",
        "2. Are the architecture parameters correct?\n",
        "3. Are the input and two target images correct?\n",
        "4. Are you able to overfit on one datapoint?\n",
        "5. Two?\n",
        "6. Are the loss functions appropriate?\n",
        "7. What do the loss values look like?\n",
        "\n",
        "### Why is my loss suddenly spiking?\n",
        "1. Learning Rate -> Might need decay\n",
        "2. Dropout is too high / in the wrong place\n",
        "3. You might be introducing NaN in your data. Check with assert not np.any(np.isnan(x))\n",
        "4. You might be having NaN due to your log loss (Solution: ??? Try softmax with logits?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VxF9dG1KU6Mf"
      },
      "source": [
        "# What about a GAN + Self correcting U-Net ? That would make for a cool architecture\n",
        "# Following CGAN -> adding a 1-hot vector encoding of the label to the training data\n",
        "# Simulated Annealing?\n",
        "# Generator -> VAE -> Discriminator?\n",
        "# What about feeding in a dicriminator's confidence level as a temperature during the autoregressive? Inverse confidence?\n",
        "# What about a 3 dimensional GAN?\n",
        "# What about adding attention to the model?\n",
        "\n",
        "# Umut Notes\n",
        "- Add a stop condition to the softmax\n",
        "    - Tried both 2 outputs and just an extra variable to the softmax\n",
        "    - 2 outputs fails due to it having too much weight to the loss and the loss fluctuates like crazy\n",
        "    - extra variable fails as the probability is still small even for an original image. Not sure why. Maybe because each time wew generate we use a new random which causes the dataset to be imbalanced?\n",
        "- 2 steps process (pick note and then choose how much through binary cross entropy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OeM4sX2NNYT0",
        "colab": {}
      },
      "source": [
        "import mnist\n",
        "import scipy.misc\n",
        "from PIL import Image\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "\n",
        "def make_grayscale(data, dtype=np.float32):\n",
        "    # luma coding weighted average in video systems\n",
        "    r, g, b = np.asarray(.3, dtype=dtype), np.asarray(.59, dtype=dtype), np.asarray(.11, dtype=dtype)\n",
        "    rst = r * data[:, :, :, 0] + g * data[:, :, :, 1] + b * data[:, :, :, 2]\n",
        "    # add channel dimension\n",
        "    rst = np.expand_dims(rst, axis=3)\n",
        "    rst = rst.astype(np.uint8)\n",
        "    return rst\n",
        "\n",
        "def create_image(image, name, image_shape, is_grayscale=False):\n",
        "    img_arr = deepcopy(image.reshape(image_shape)).astype(np.uint8)\n",
        "    img_arr = np.squeeze(img_arr)\n",
        "\n",
        "    if is_grayscale:\n",
        "        img = Image.fromarray(img_arr.astype(np.uint8), 'L')\n",
        "    else:\n",
        "        img = Image.fromarray(img_arr.astype(np.uint8), 'RGB')\n",
        "    # pprint(img_arr)\n",
        "    # print(\"img shape: {}. img sum: {}\".format(img_arr.shape, img_arr.sum()))\n",
        "    img.save(name)\n",
        "    return img\n",
        "\n",
        "is_single = True\n",
        "is_grayscale = False  # False for Color\n",
        "is_cifar_10 = True\n",
        "\n",
        "if is_single:\n",
        "    num_samples = 10\n",
        "    epochs_per_sample = 1000\n",
        "    num_sub_layers = 1\n",
        "else:\n",
        "    num_samples = 60000\n",
        "    epochs_per_sample = 20\n",
        "    num_sub_layers = 1\n",
        "\n",
        "if is_cifar_10:\n",
        "    (images, labels), (_, _) = cifar10.load_data()\n",
        "    if is_grayscale:\n",
        "        images = make_grayscale(images)\n",
        "else:\n",
        "    images = mnist.train_images()\n",
        "\n",
        "    # np.random.shuffle(images)\n",
        "images = images[:num_samples, :, :]\n",
        "\n",
        "# pprint(images)\n",
        "print(images.shape)\n",
        "\n",
        "# labels = mnist.train_labels()\n",
        "# n_labels = np.max(labels) + 1\n",
        "# labels = np.eye(n_labels)[labels]\n",
        "# print(labels.shape)\n",
        "\n",
        "if is_cifar_10:\n",
        "    image_shape = images[0].shape\n",
        "else:\n",
        "    image_shape = np.expand_dims(images[0], axis=-1).shape \n",
        "\n",
        "print(images[0])\n",
        "create_image(images[0], 'my.png', image_shape, is_grayscale=is_grayscale)\n",
        "create_image(images[-1], 'my2.png', image_shape, is_grayscale=is_grayscale)\n",
        "print(image_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIaOI3kjQB60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_one_hot(arr):\n",
        "    arr = deepcopy(arr)\n",
        "    arr = arr.astype(np.uint8)\n",
        "    n_values = 256\n",
        "    one_hot = np.eye(n_values)[arr]\n",
        "    one_hot = one_hot.astype(np.uint8)\n",
        "    return one_hot\n",
        "\n",
        "one_hot = to_one_hot(images[0])\n",
        "print(one_hot.shape)\n",
        "\n",
        "argmax_res = np.argmax(one_hot, axis=-1)\n",
        "print(argmax_res)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OlmEDomwi9dZ",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Flatten, Dense, Softmax, Reshape, Activation\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model\n",
        "from keras import backend as K\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "def built_in_softmax_kl_loss(target, output):\n",
        "    target = K.flatten(target)\n",
        "    output = K.flatten(output)\n",
        "    \n",
        "    target = target / K.sum(target)\n",
        "    output = K.softmax(output)\n",
        "    return keras.losses.kullback_leibler_divergence(target, output)\n",
        "\n",
        "def intensity_softmax_loss(target, output):\n",
        "    return keras.losses.categorical_crossentropy(target, output, from_logits=True)\n",
        "\n",
        "keras.losses.built_in_softmax_kl_loss = built_in_softmax_kl_loss\n",
        "keras.losses.intensity_softmax_loss = intensity_softmax_loss\n",
        "\n",
        "def conv_layer(n_filters, filter_size, conv):\n",
        "    conv = Conv2D(n_filters, filter_size, activation='relu', padding='same')(conv)\n",
        "    conv = Conv2D(n_filters, filter_size, activation='relu', padding='same')(conv)\n",
        "    conv = Conv2D(n_filters, filter_size, activation='relu', padding='same')(conv)\n",
        "    return conv    \n",
        " \n",
        "def unet_model(input_size=(28, 28, 1), n_filters_start=32, growth_factor=2,\n",
        "               upconv=False, is_grayscale=True, num_sub_layers=1):\n",
        "    droprate=0.5\n",
        "    n_filters = n_filters_start\n",
        "    inputs = Input(input_size)\n",
        "    conv_first = conv_layer(n_filters, (3, 3), inputs)\n",
        "    pool_first = MaxPooling2D(pool_size=(2, 2))(conv_first)\n",
        "\n",
        "    prev_pool = pool_first\n",
        "    hidden_layers = []\n",
        "    for _ in range(num_sub_layers):\n",
        "        n_filters *= growth_factor\n",
        "        pool = BatchNormalization()(prev_pool)\n",
        "        conv = conv_layer(n_filters, (3, 3), pool)\n",
        "        pool = MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "        pool = Dropout(droprate)(pool)\n",
        "        prev_pool = pool\n",
        "        hidden_layers.append(conv)\n",
        " \n",
        "    n_filters *= growth_factor\n",
        "    conv_mid = conv_layer(n_filters, (3, 3), prev_pool)\n",
        "    # print(hidden_layers)\n",
        " \n",
        "    n_filters //= growth_factor\n",
        "    if upconv:\n",
        "        up_first = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv_mid), hidden_layers[-1]])\n",
        "    else:\n",
        "        up_first = concatenate([UpSampling2D(size=(2, 2))(conv_mid), hidden_layers[-1]])\n",
        "    up_first = BatchNormalization()(up_first)\n",
        "    conv_mid_2 = conv_layer(n_filters, (3, 3), up_first)\n",
        "    conv_mid_2 = Dropout(droprate)(conv_mid_2)\n",
        "\n",
        "    prev_conv = conv_mid_2\n",
        "    for i in range(num_sub_layers - 1):\n",
        "        n_filters //= growth_factor\n",
        "        if upconv:\n",
        "            up = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(prev_conv), hidden_layers[-i-2]])\n",
        "        else:\n",
        "            up = concatenate([UpSampling2D(size=(2, 2))(prev_conv), hidden_layers[-i-2]])\n",
        "        up = BatchNormalization()(up)\n",
        "        conv = conv_layer(n_filters, (3, 3), up)\n",
        "        conv = Dropout(droprate)(conv)\n",
        "        prev_conv = conv\n",
        " \n",
        "    n_filters //= growth_factor\n",
        "    if upconv:\n",
        "        up_last = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(prev_conv), conv_first])\n",
        "    else:\n",
        "        up_last = concatenate([UpSampling2D(size=(2, 2))(prev_conv), conv_first])\n",
        "    conv_last = conv_layer(n_filters, (3, 3), up_last)\n",
        " \n",
        "    softmax_out = Conv2D(1, 1, activation='linear', name='softmax_out')(conv_last)\n",
        "\n",
        "    if is_grayscale:\n",
        "        sigmoid_out = Conv2D(256, 1, padding='valid', name='sigmoid_out')(conv_last)\n",
        "        sigmoid_out = Reshape((*image_shape[:-1], 256))(sigmoid_out)\n",
        "        # sigmoid_out = Activation('softmax')(sigmoid_out)\n",
        "        model = Model(inputs=inputs, outputs=[softmax_out, sigmoid_out])\n",
        "        model.compile(optimizer=Adam(lr=0.001), loss=[built_in_softmax_kl_loss, intensity_softmax_loss], metrics=['categorical_accuracy'])\n",
        "    else:\n",
        "        intensity_softmax = Conv2D(256 * 3, 1, padding='valid', name='intensity_conv')(conv_last)\n",
        "        intensity_softmax = Reshape((*image_shape, 256))(intensity_softmax)\n",
        "        # intensity_softmax = Activation('softmax', name='intensity_softmax')(intensity_softmax)\n",
        "        model = Model(inputs=inputs, outputs=[softmax_out, intensity_softmax])\n",
        "        model.compile(optimizer=Adam(lr=0.001), loss=[built_in_softmax_kl_loss, intensity_softmax_loss], metrics=['categorical_accuracy'])\n",
        "\n",
        "    # model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rSjQr52iQzlu",
        "colab": {}
      },
      "source": [
        "model = unet_model(input_size=image_shape, is_grayscale=is_grayscale, num_sub_layers=num_sub_layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsFM5XMYeqBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBEdqEBxFqNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# discriminator_model = discriminator(input_size=image_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SVUVU8Kt_aCm",
        "colab": {}
      },
      "source": [
        "from copy import deepcopy\n",
        "import math\n",
        "import itertools\n",
        "import time\n",
        "import random\n",
        "\n",
        "noise_upper_bound = 2.5\n",
        "\n",
        "def mask_image_with_noise(image, is_grayscale=True):\n",
        "    image = deepcopy(image)\n",
        "    sampling_percentage_mask = np.random.uniform(0, 100)\n",
        "    sampling_percentage_noise = np.random.uniform(0, noise_upper_bound)\n",
        "    pixel_count = np.prod(image.shape[:-1])\n",
        "    mask = np.full(image.shape[:-1], False).flatten()\n",
        "    noise = np.full(image.shape[:-1], False).flatten()\n",
        "    amount_to_mask = math.floor(pixel_count * (sampling_percentage_mask / 100.0))\n",
        "    mask[:amount_to_mask] = True\n",
        "    amount_of_noise = math.floor(pixel_count * (sampling_percentage_noise / 100.0))\n",
        "    noise[:amount_of_noise] = True\n",
        "    np.random.shuffle(mask)\n",
        "    np.random.shuffle(noise)\n",
        "    # Take into account the values that are already 0\n",
        "    if is_grayscale:\n",
        "        image = image.flatten()\n",
        "        mask[image == 0] = False\n",
        "        noise[image == 0] = False\n",
        "    else:\n",
        "        argmax_image = np.sum(image, axis=-1)\n",
        "        argmax_image = argmax_image.flatten()\n",
        "        mask[argmax_image == 0] = False\n",
        "        noise[argmax_image == 0] = False\n",
        "\n",
        "    output_image = deepcopy(image)\n",
        "    xor_target = np.full(pixel_count, False).flatten()\n",
        "\n",
        "    if is_grayscale:\n",
        "        output_image = output_image.flatten()\n",
        "        output_image[mask] = 0\n",
        "        xor_target[mask] = True\n",
        "    else:\n",
        "        output_image = output_image.reshape(-1, 3)\n",
        "        output_image[mask, :] = 0\n",
        "        xor_target[mask] = True\n",
        "\n",
        "    # There might be overlap but that is ok\n",
        "    random_values = np.random.uniform(0, 1, image.shape).flatten()\n",
        "    random_values *= 255\n",
        "    random_values = np.around(random_values)\n",
        "    random_values = random_values.astype(np.uint8)\n",
        "    if is_grayscale:\n",
        "        random_values = random_values[:np.sum(noise)]\n",
        "        output_image[noise] = random_values\n",
        "        xor_target[noise] = True\n",
        "    else:\n",
        "        output_image = output_image.reshape(-1, 3)\n",
        "        random_values = random_values[:np.sum(noise) * 3].reshape(-1, 3)\n",
        "        output_image[noise, :] = random_values\n",
        "        xor_target[noise] = True\n",
        "\n",
        "    output_image = output_image.reshape(image.shape)\n",
        "    xor_target = xor_target.reshape(image.shape[:-1])\n",
        "    xor_target = np.expand_dims(xor_target, axis=-1)\n",
        "    return output_image, xor_target\n",
        "\n",
        "\n",
        "class ImageGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, sample_list, image_shape, batch_size, samples_per_data_item, stops_per_data_item, is_grayscale=True, seed=None):\n",
        "        print(\"sample_list: {}\".format(len(sample_list)))\n",
        "        self.sample_list = sample_list\n",
        "        self.image_shape = image_shape\n",
        "        self.batch_size = batch_size\n",
        "        self.samples_per_data_item = samples_per_data_item\n",
        "        self.stops_per_data_item = stops_per_data_item\n",
        "        self.is_grayscale = is_grayscale\n",
        "        self.sample_index = 0\n",
        "        self.seed = seed\n",
        "        self.dtype = np.uint8\n",
        "        # if self.seed is not None:\n",
        "        #     np.random.seed(self.seed)\n",
        "\n",
        "    def generate_training_pairs(self):\n",
        "        '''\n",
        "        Generates Training Pairs till @training_input / @training_target have @batch_size files.\n",
        "        '''\n",
        "        training_input = []\n",
        "        training_original = []\n",
        "        training_target = []\n",
        "        while len(training_input) < self.batch_size:\n",
        "            original_image = deepcopy(self.sample_list[self.sample_index])\n",
        "            original_image = original_image.reshape(self.image_shape)\n",
        "            binary_image = deepcopy(original_image)\n",
        "            binary_image[binary_image > 0] = 1\n",
        "            self.sample_index = (self.sample_index + 1) % len(self.sample_list)\n",
        "            # print(\"sample_list length: {}. sample_index: {}\".format(\n",
        "            #     len(self.sample_list), self.sample_index))\n",
        "            try:\n",
        "                # augment by adding and removing random values in the array\n",
        "\n",
        "                # Add random values\n",
        "                for _ in range(self.samples_per_data_item):\n",
        "                    original_image = original_image.astype(self.dtype)\n",
        "                    input_image, xor_target = mask_image_with_noise(original_image, is_grayscale=self.is_grayscale)\n",
        "\n",
        "                    input_image = input_image.astype(self.dtype)\n",
        "                    xor_target = xor_target.astype(self.dtype)\n",
        "\n",
        "                    training_input.append(deepcopy(input_image))\n",
        "                    training_original.append(to_one_hot(np.squeeze(original_image)))\n",
        "                    # training_original.append(deepcopy(original_image))\n",
        "                    training_target.append(deepcopy(xor_target))\n",
        "\n",
        "            except Exception as e:\n",
        "                print('Error generating input and target pair')\n",
        "                traceback.print_exc()\n",
        "        training_input = np.asarray(training_input)\n",
        "        training_target = np.asarray(training_target)\n",
        "        training_original = np.asarray(training_original)\n",
        "        return training_input, training_target, training_original\n",
        "\n",
        "    def save_image(self, img_arr, img_name, is_target=False):\n",
        "        # img_arr = img_arr.reshape(self.image_shape)\n",
        "        print(img_name)\n",
        "        print(\"img shape: {}. img sum: {}\".format(img_arr.shape, img_arr.sum()))\n",
        "        img_arr = np.squeeze(img_arr)\n",
        "        print(\"img shape: {}. img sum: {}\".format(img_arr.shape, img_arr.sum()))\n",
        "        print(img_arr)\n",
        "        #pprint(img_arr)\n",
        "        print(\"img shape: {}. img sum: {}\".format(img_arr.shape, img_arr.sum()))\n",
        "        if self.is_grayscale or is_target:\n",
        "            if is_target:\n",
        "                img_arr *= 255\n",
        "            img = Image.fromarray(img_arr.astype(np.uint8), 'L')\n",
        "        else:\n",
        "            img = Image.fromarray(img_arr.astype(np.uint8), 'RGB')\n",
        "\n",
        "        img.save(img_name)\n",
        "\n",
        "    def get_random_training_pair(self):\n",
        "        training_input, training_target, training_original = self.generate_training_pairs()\n",
        "        print(\"training_input shape: {}\".format(training_input.shape))\n",
        "        index = random.randrange(0, len(training_input))\n",
        "        self.save_image(deepcopy(training_input[index]), 'training_input.png')\n",
        "        self.save_image(deepcopy(training_target[index]), 'training_target.png', is_target=True)\n",
        "        print(training_original.shape)\n",
        "        original_image = deepcopy(training_original[index])\n",
        "        original_image = np.argmax(original_image, axis=-1)\n",
        "        print(original_image.shape)\n",
        "        original_image = np.expand_dims(original_image, axis=-1)\n",
        "        self.save_image(original_image, 'training_original.png')\n",
        "\n",
        "    def generate_validation_samples(self):\n",
        "        old_batch_size = self.batch_size\n",
        "        self.batch_size = len(self.sample_list) * (self.samples_per_data_item + self.stops_per_data_item)\n",
        "        training_input, training_target, training_original = self.generate_training_pairs()\n",
        "        # training_input = np.asarray(self.training_input[:self.batch_size])\n",
        "        # training_target = np.asarray(self.training_target[:self.batch_size])\n",
        "        self.batch_size = old_batch_size\n",
        "        return training_input, [training_target, training_original]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''Generates 1 batch of data'''\n",
        "        training_input, training_target, training_original = self.generate_training_pairs()\n",
        "        return training_input, [training_target, training_original]\n",
        "\n",
        "    def __len__(self):\n",
        "        '''Number of batches / epoch'''\n",
        "        # print(\"sample_list: {}. samples_per_data_item: {}, batch size: {}\".\n",
        "        #       format(len(self.sample_list), self.samples_per_data_item,\n",
        "        #              self.batch_size))\n",
        "        samples_to_generate = int(\n",
        "            (len(self.sample_list) * (self.samples_per_data_item + self.stops_per_data_item)) /\n",
        "            self.batch_size)\n",
        "        # print(\"samples to generate: {}\".format(samples_to_generate))\n",
        "        return samples_to_generate\n",
        "    \n",
        "    # def on_epoch_begin(self):\n",
        "    #     if self.seed is not None:\n",
        "    #         np.random.seed(self.seed)\n",
        "    #     else:\n",
        "    #         np.random.seed(time.time())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1zwLYew1a-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Config\n",
        "stops_per_data_item = 0\n",
        "if is_single:\n",
        "    batch_size = num_samples * 32\n",
        "    samples_per_data_item = 1 * 32\n",
        "    split = 1\n",
        "else:\n",
        "    batch_size = 128\n",
        "    samples_per_data_item = 1\n",
        "    split = 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ngo7o_rw2TsK",
        "colab": {}
      },
      "source": [
        "training_samples = images[:int(len(images) * split)]\n",
        "validation_samples = images[int(len(images) * split):]\n",
        "\n",
        "print(\"training samples: {}. validation samples: {}\".format(len(training_samples), len(validation_samples)))\n",
        "\n",
        "steps_per_epoch = int(len(training_samples) * (samples_per_data_item + stops_per_data_item) / batch_size)\n",
        "print(\"steps per epoch: {}\".format(steps_per_epoch))\n",
        "\n",
        "# pprint(training_samples[0])\n",
        "\n",
        "training_generator = ImageGenerator(\n",
        "    sample_list=training_samples,\n",
        "    image_shape=image_shape,\n",
        "    batch_size=batch_size,\n",
        "    samples_per_data_item=samples_per_data_item,\n",
        "    stops_per_data_item=stops_per_data_item,\n",
        "    is_grayscale=is_grayscale)\n",
        "\n",
        "validation_generator = ImageGenerator(\n",
        "    sample_list=validation_samples,\n",
        "    image_shape=image_shape,\n",
        "    batch_size=batch_size,\n",
        "    samples_per_data_item=samples_per_data_item,\n",
        "    stops_per_data_item=stops_per_data_item,\n",
        "    is_grayscale=is_grayscale)\n",
        "\n",
        "validation_data = validation_generator.generate_validation_samples()\n",
        "\n",
        "# print(\"validation data input and target shape: {}\".format(validation_data[0].shape))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsbSXfJcezGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_generator.get_random_training_pair()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk-zvNP9DGWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if is_single:\n",
        "    is_single_text = \"single\"\n",
        "else:\n",
        "    is_single_text = \"full\"\n",
        "\n",
        "model_custom_name = 'cifar-grayscale-double-softmax'\n",
        "model_full_name = '{}-num-samples-{}-noise-upper-{}-num-sub-layers-{}-{}'.format(model_custom_name, num_samples, noise_upper_bound, num_sub_layers, is_single_text)\n",
        "model_location = '/content/drive/My Drive/checkpoints/{}.hdf5'.format(model_full_name)\n",
        "log_dir = '/content/drive/My Drive/logs/{}'.format(model_full_name)\n",
        "print(log_dir)\n",
        "print(model_location)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkGTGGr06HUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# updatable plot\n",
        "# a minimal example\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class PlotLosses(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        \n",
        "        self.fig = plt.figure()\n",
        "        \n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(self.i)\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.i += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        plt.plot(self.x, self.losses, label=\"loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
        "        plt.legend()\n",
        "        # plt.xscale('log')\n",
        "        plt.show();\n",
        "        \n",
        "plot_losses = PlotLosses()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCpnRH7bs0hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "class EvaluateCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, image_shape, sample_dir):\n",
        "        self.image_shape = image_shape\n",
        "        self.sample_dir = sample_dir\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if epoch % epochs_per_sample == 0:\n",
        "            sample_sqrt = 2\n",
        "            generated_images = []\n",
        "            for i in range(sample_sqrt**2):\n",
        "                directory = \"images_{}\".format(i)\n",
        "                os.makedirs(directory, exist_ok=True)\n",
        "                input_image = self.generate_noise()\n",
        "\n",
        "                img, _ = self.inference(model, input_image, directory, 3000)\n",
        "                generated_images.append(img)\n",
        "          \n",
        "            final_im = Image.new('RGB', (image_shape[0] * sample_sqrt, image_shape[1] * sample_sqrt))\n",
        "\n",
        "            y_offset = 0\n",
        "            for i in range(sample_sqrt):\n",
        "                x_offset = 0\n",
        "                new_im = Image.new('RGB', (image_shape[0] * sample_sqrt, image_shape[1]))\n",
        "                for j in range(sample_sqrt):\n",
        "                    im = deepcopy(generated_images[(i * sample_sqrt) + j])\n",
        "                    new_im.paste(im, (x_offset, 0))\n",
        "                    x_offset += image_shape[0]\n",
        "                final_im.paste(new_im, (0, y_offset))\n",
        "                y_offset += image_shape[0]\n",
        "                \n",
        "            os.makedirs(self.sample_dir, exist_ok=True)\n",
        "            final_im.save(os.path.join(self.sample_dir, 'samples_epoch_{}.png'.format(epoch)))\n",
        "\n",
        "\n",
        "    def generate_noise(self):\n",
        "        input_image = np.full(self.image_shape, 0)\n",
        "        input_image = input_image.astype(np.uint8)\n",
        "        input_image = np.expand_dims(input_image, 0)\n",
        "        return input_image\n",
        "\n",
        "    def inference(self, model, input_image, directory, iterations):        \n",
        "        working_image = deepcopy(input_image)\n",
        "\n",
        "        for i in range(iterations):\n",
        "            softmax_predictions, sigmoid_predictions = model.predict(working_image)\n",
        "            softmax_predictions = softmax_predictions.flatten()\n",
        "\n",
        "            softmax_predictions = softmax_predictions - np.max(softmax_predictions)\n",
        "            softmax_predictions = np.exp(softmax_predictions)\n",
        "            softmax_predictions = softmax_predictions / np.sum(softmax_predictions)\n",
        "            indices = np.arange(softmax_predictions.shape[0])\n",
        "\n",
        "            index = np.random.choice(indices, p=softmax_predictions)\n",
        "\n",
        "            if is_grayscale:\n",
        "                sigmoid_predictions = sigmoid_predictions.reshape(-1, 256)\n",
        "                working_image = working_image.flatten()\n",
        "            else:\n",
        "                sigmoid_predictions = sigmoid_predictions.reshape(-1, 3, 256)\n",
        "                working_image = working_image.reshape(-1, 3)\n",
        "\n",
        "            if is_grayscale:\n",
        "                sigmoid_probs = sigmoid_predictions[index]\n",
        "                sigmoid_probs = sigmoid_probs - np.max(sigmoid_probs)\n",
        "                sigmoid_probs = np.exp(sigmoid_probs)\n",
        "                sigmoid_probs = sigmoid_probs / np.sum(sigmoid_probs)\n",
        "                sigmoid_indices = np.arange(sigmoid_probs.shape[0])\n",
        "                working_image[index] = np.random.choice(sigmoid_indices, p=sigmoid_probs)\n",
        "            else:\n",
        "                for channel in range(3):\n",
        "                    sigmoid_probs = sigmoid_predictions[index, channel]\n",
        "                    sigmoid_probs = sigmoid_probs - np.max(sigmoid_probs)\n",
        "                    sigmoid_probs = np.exp(sigmoid_probs)\n",
        "                    sigmoid_probs = sigmoid_probs / np.sum(sigmoid_probs)\n",
        "                    sigmoid_indices = np.arange(sigmoid_probs.shape[0])\n",
        "                    chosen_index = np.random.choice(sigmoid_indices, p=sigmoid_probs)\n",
        "                    working_image[index, channel] = chosen_index\n",
        "\n",
        "            working_image = np.reshape(working_image, [1, *self.image_shape])\n",
        "\n",
        "        final_image = working_image\n",
        "        img = create_image(final_image, os.path.join(directory, 'final.png'), image_shape=self.image_shape)\n",
        "        return img, deepcopy(final_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dQKfsm7sEpV0",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "model = unet_model(input_size=image_shape, is_grayscale=is_grayscale, num_sub_layers=num_sub_layers)\n",
        "\n",
        "resume_training = False\n",
        "if resume_training:\n",
        "    model = keras.models.load_model(model_location)\n",
        "\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=model_location,\n",
        "    monitor='val_loss',\n",
        "    save_weights_only=False,\n",
        "    verbose=1,\n",
        "    mode='min',\n",
        "    save_best_only=False)\n",
        "\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, update_freq='epoch')\n",
        "evaluate_callback = EvaluateCallback(image_shape, log_dir)\n",
        "\n",
        "if True:\n",
        "    if is_single:\n",
        "        history = model.fit(\n",
        "            training_generator,\n",
        "            # validation_data=validation_data,\n",
        "            verbose=1,\n",
        "            shuffle=True,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=30000,\n",
        "            callbacks=[model_checkpoint_callback, evaluate_callback, tensorboard_callback])#, tensorboard_callback])\n",
        "    else:\n",
        "        history = model.fit(\n",
        "            training_generator,\n",
        "            validation_data=validation_data,\n",
        "            verbose=1,\n",
        "            shuffle=True,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=1000,\n",
        "            callbacks=[model_checkpoint_callback, plot_losses, tensorboard_callback, evaluate_callback])#, tensorboard_callback])\n",
        "    #epochs=cfg.epochs,\n",
        "    #callbacks=callbacks)\n",
        "# model.save(\"sc-model.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU7a7s_5JRMR",
        "colab_type": "text"
      },
      "source": [
        "Current experiment: 10 data items Only 30% accuracy T.T.\n",
        "10000 epochs and 40% accuracy... Grayscale was able to hit 80% accuracy at 50000 epochs. Loss oscillates a lot.... maybe I need LR decay? Maybe a larger model?\n",
        "\n",
        "# Experiments\n",
        "\n",
        "I tried changing from softmax + categorical_cross entropy into categorical_cross_entropy with logits which is the same as softmax_nn with logits. This fixes some log instability that occurs when training for a long time.\n",
        "\n",
        "10 samples works fairly well for orderless NADE w/o noise! only reached around 42% accuracy at 20000 epochs, but it just means the images are slightly blurry for right now.\n",
        "\n",
        "trying 10 samples with 2.5 % noise! ~30000 epochs. nade-cifar-color-double-softmax-num-samples-10-noise-upper-2.5-single.hdf5. Noticed that this model converges ~half has fast as the model for grayscale.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-51yFm3JLzv",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKxrjiG_eqBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"Loss\"\n",
        "if True:\n",
        "    plt.plot(history.history['loss'])\n",
        "    if not is_single:\n",
        "        plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train'], loc='upper left')\n",
        "    # plt.xscale('log')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0IEPAcide5lS"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF8wPwoAo_xp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# testing model predict with seaborn and plots\n",
        "model = keras.models.load_model(model_location)\n",
        "import seaborn as  sb\n",
        "import matplotlib.pyplot as plt\n",
        "print(image_shape)\n",
        "def generate_noise():\n",
        "    input_image = np.full(image_shape, 0)\n",
        "\n",
        "    input_image = input_image.astype(np.uint8)\n",
        "    input_image = np.expand_dims(input_image, 0)\n",
        "    return input_image\n",
        "\n",
        "test_image = generate_noise()\n",
        "softmax_predictions, sigmoid_predictions = model.predict(test_image)\n",
        "softmax_predictions = softmax_predictions.reshape(image_shape[0], image_shape[1])\n",
        "heatmap = sb.heatmap(softmax_predictions)\n",
        "plt.show()\n",
        "softmax_predictions = np.exp(softmax_predictions)\n",
        "softmax_predictions = softmax_predictions / np.sum(softmax_predictions)\n",
        "print(sigmoid_predictions.shape)\n",
        "# print(sigmoid_predictions[])\n",
        "sigmoid_predictions = np.argmax(sigmoid_predictions, axis=-1)\n",
        "print(sigmoid_predictions.shape)\n",
        "sigmoid_predictions = np.squeeze(sigmoid_predictions)\n",
        "heatmap = sb.heatmap(softmax_predictions)\n",
        "plt.show()\n",
        "if is_grayscale:\n",
        "    heatmap = sb.heatmap(sigmoid_predictions)\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PBu4a18br7wL",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "\n",
        "def inference(model, input_image, directory, iterations, temp_start=2, temp_end=0.5, top_k=250, is_grayscale=True, is_debug=False):\n",
        "    create_image(input_image, \"{}/input.png\".format(directory), image_shape=image_shape)\n",
        "\n",
        "    # temperatures = np.linspace(temp_start, temp_end, num=iterations)\n",
        "    temperatures = np.geomspace(temp_start, temp_end, num=iterations)\n",
        "    temperatures_reverse = (temp_start + temp_end) - temperatures[::-1]\n",
        "    temperatures = np.concatenate((temperatures_reverse[:int(temperatures.shape[0]/2)], temperatures[int(temperatures.shape[0]/2):]))\n",
        "    \n",
        "    working_image = deepcopy(input_image)\n",
        "    working_images = []\n",
        "    num_added = 0\n",
        "    num_removed = 0\n",
        "    for i in range(iterations):\n",
        "        temp = temperatures[i]            \n",
        "        binary_image = deepcopy(working_image)\n",
        "        binary_image[binary_image > 0] = 1\n",
        "        softmax_predictions, sigmoid_predictions = model.predict(working_image)\n",
        "\n",
        "        softmax_predictions = softmax_predictions.flatten()\n",
        "\n",
        "        softmax_predictions = softmax_predictions - np.max(softmax_predictions)\n",
        "        softmax_predictions = np.exp(softmax_predictions / temp)\n",
        "        softmax_predictions = softmax_predictions / np.sum(softmax_predictions)\n",
        "        indices = np.arange(softmax_predictions.shape[0])\n",
        "\n",
        "        # zipped = zip(softmax_predictions, indices)\n",
        "        # zipped = list(reversed(sorted(zipped, key = lambda x : x[0])))\n",
        "        # zipped = zipped[:top_k]\n",
        "        # zipped = sorted(zipped, key=lambda x : x[1])\n",
        "        # softmax_predictions, indices = zip(*zipped)\n",
        "        # softmax_predictions = np.asarray(softmax_predictions)\n",
        "        # softmax_predictions = softmax_predictions / np.sum(softmax_predictions)\n",
        "        # indices = np.asarray(indices)\n",
        "\n",
        "        index = np.random.choice(indices, p=softmax_predictions)\n",
        "\n",
        "        if is_grayscale:\n",
        "            # sigmoid_predictions = np.argmax(sigmoid_predictions, axis=-1)\n",
        "            sigmoid_predictions = sigmoid_predictions.reshape(-1, 256)\n",
        "            working_image = working_image.flatten()\n",
        "        else:\n",
        "            sigmoid_predictions = sigmoid_predictions.reshape(-1, 3, 256)\n",
        "            working_image = working_image.reshape(-1, 3)\n",
        "\n",
        "        if np.sum(working_image[index]) != 0:\n",
        "            num_removed += 1\n",
        "        elif np.sum(working_image[index]) == 0:\n",
        "            num_added += 1\n",
        "        if is_grayscale:\n",
        "            sigmoid_probs = sigmoid_predictions[index]\n",
        "            sigmoid_probs = sigmoid_probs - np.max(sigmoid_probs)\n",
        "            sigmoid_probs = np.exp(sigmoid_probs)\n",
        "            sigmoid_probs = sigmoid_probs / np.sum(sigmoid_probs)\n",
        "            sigmoid_indices = np.arange(sigmoid_probs.shape[0])\n",
        "            working_image[index] = np.random.choice(sigmoid_indices, p=sigmoid_probs)\n",
        "        else:\n",
        "            for channel in range(3):\n",
        "                sigmoid_probs = sigmoid_predictions[index, channel]\n",
        "                sigmoid_probs = sigmoid_probs - np.max(sigmoid_probs)\n",
        "                sigmoid_probs = np.exp(sigmoid_probs)\n",
        "                sigmoid_probs = sigmoid_probs / np.sum(sigmoid_probs)\n",
        "                sigmoid_indices = np.arange(sigmoid_probs.shape[0])\n",
        "                chosen_index = np.random.choice(sigmoid_indices, p=sigmoid_probs)\n",
        "                working_image[index, channel] = chosen_index\n",
        "\n",
        "        working_image = np.reshape(working_image, [1, *image_shape])\n",
        "        if i % 50 == 0:\n",
        "            if is_debug:\n",
        "                print(\"softmax\")\n",
        "                softmax_predictions = softmax_predictions.reshape(image_shape[:-1])\n",
        "                heatmap = sb.heatmap(deepcopy(softmax_predictions))\n",
        "                plt.show()\n",
        "                # print(\"sigmoid\")\n",
        "                # sigmoid_predictions = np.argmax(sigmoid_predictions, axis=-1).reshape(image_shape[:-1])\n",
        "                # heatmap = sb.heatmap(deepcopy(sigmoid_predictions))\n",
        "                # plt.show()\n",
        "            create_image(working_image, os.path.join(directory, \"working_{}.png\".format(i)), image_shape=image_shape)\n",
        "\n",
        "    final_image = working_image\n",
        "    final_binary_image = deepcopy(final_image)\n",
        "    final_binary_image[final_binary_image > 0] = 1\n",
        "    create_image(final_binary_image, os.path.join(directory, \"final_binary.png\"), image_shape=image_shape)\n",
        "\n",
        "    print(final_image.shape)\n",
        "    print(\"num added: {}. num fixed: {}\".format(num_added, num_removed))\n",
        "    img = create_image(final_image, os.path.join(directory, 'final.png'), image_shape=image_shape)\n",
        "    return img, deepcopy(final_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC6ZVFAFeqBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = keras.models.load_model(model_location)\n",
        "\n",
        "drive_folder = '/content/drive/My Drive'\n",
        "\n",
        "current_model_name = model_location.split('My Drive/')[-1].split('.hdf5')[0]\n",
        "\n",
        "model_names = [current_model_name,\n",
        "               # 'sc-model-es-net-60000-4',\n",
        "               #'sc-model-es-net-60000-16',\n",
        "               # 'sc-model-es-net-mnist-grayscale-double-softmax',\n",
        "               #'sc-model-nade-60000-4'\n",
        "               # 'checkpoints/nade-cifar-color-double-softmax-0-single'\n",
        "               # 'checkpoints/nade-cifar-color-double-softmax-num-samples-10-noise-upper-0-single'\n",
        "               # 'checkpoints/nade-cifar-color-double-softmax-num-samples-1-noise-upper-0-single'\n",
        "               # 'checkpoints/model_full_name'\n",
        "               ]\n",
        "\n",
        "config = {\n",
        "    'sc-model-es-net-60000-4': {\n",
        "        \"iterations\": 300,\n",
        "        \"temp_start\": 0.99,\n",
        "        \"temp_end\": 0.99,\n",
        "        \"top_k\": 10000\n",
        "    },\n",
        "    current_model_name: {\n",
        "        \"iterations\": 1500,\n",
        "        \"temp_start\": 1,\n",
        "        \"temp_end\": 1,\n",
        "        \"top_k\": 10000\n",
        "    },\n",
        "    'checkpoints/nade-cifar-color-double-softmax-num-samples-10-noise-upper-0-single': {\n",
        "        \"iterations\": 1500,\n",
        "        \"temp_start\": 1,\n",
        "        \"temp_end\": 1,\n",
        "        \"top_k\": 10000\n",
        "    },\n",
        "    'checkpoints/nade-cifar-color-double-softmax-num-samples-1-noise-upper-0-single': {\n",
        "        \"iterations\": 1500,\n",
        "        \"temp_start\": 1,\n",
        "        \"temp_end\": 1,\n",
        "        \"top_k\": 10000\n",
        "    },\n",
        "    'checkpoints/sc-model-es-net-cifar-color-double-softmax-1-single': {\n",
        "        \"iterations\": 3000,\n",
        "        \"temp_start\": 1,\n",
        "        \"temp_end\": 1,\n",
        "        \"top_k\": 10000\n",
        "    },\n",
        "    'checkpoints/model_full_name': {\n",
        "        \"iterations\": 3000,\n",
        "        \"temp_start\": 0.99,\n",
        "        \"temp_end\": 0.99,\n",
        "        \"top_k\" : 10000\n",
        "    },\n",
        "\n",
        "}\n",
        "\n",
        "sample_sqrt = 3\n",
        "for model_name in model_names:\n",
        "    model = keras.models.load_model(os.path.join(drive_folder, model_name + '.hdf5'))\n",
        "    # model = keras.models.load_model(F'/content/drive/My Drive/checkpoints/model_full_name.hdf5')\n",
        "    model_config = config[model_name]\n",
        "    generated_images = []\n",
        "    for i in range(sample_sqrt**2):\n",
        "        directory = \"images_{}\".format(i)\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "        input_image = generate_noise()\n",
        "        # input_image = np.expand_dims(np.expand_dims(images[i], 0), -1)\n",
        "\n",
        "        img, _ = inference(model, input_image, directory, \n",
        "                           model_config['iterations'], temp_start=model_config['temp_start'], \n",
        "                           temp_end=model_config['temp_end'], top_k=model_config['top_k'], \n",
        "                           is_grayscale=is_grayscale, is_debug=False)\n",
        "        generated_images.append(img)\n",
        "    \n",
        "    final_im = Image.new('RGB', (image_shape[0] * sample_sqrt, image_shape[1] * sample_sqrt))\n",
        "\n",
        "    y_offset = 0\n",
        "    for i in range(sample_sqrt):\n",
        "        x_offset = 0\n",
        "        new_im = Image.new('RGB', (image_shape[0] * sample_sqrt, image_shape[1]))\n",
        "        for j in range(sample_sqrt):\n",
        "            im = deepcopy(generated_images[(i * sample_sqrt) + j])\n",
        "            new_im.paste(im, (x_offset, 0))\n",
        "            x_offset += image_shape[0]\n",
        "        final_im.paste(new_im, (0, y_offset))\n",
        "        y_offset += image_shape[0]\n",
        "        \n",
        "    model_name = model_name.split('/')[-1]\n",
        "    final_im.save(model_name + '.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wItUEoqT-9Qs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}