{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SC_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waynchi/SC-Net/blob/master/SC_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U22qZkbsfEyI",
        "colab_type": "code",
        "outputId": "f0de004a-e2ce-43b5-9246-f7bc9ab49c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!pip install mnist\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mnist in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mnist) (1.18.4)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45bLorRNFfdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LsQqrsgeqA1",
        "colab_type": "code",
        "outputId": "48f72b06-e712-4c32-929c-b70037367079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.is_gpu_available()\n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VxF9dG1KU6Mf"
      },
      "source": [
        "# What about a GAN + Self correcting U-Net ? That would make for a cool architecture\n",
        "# Following CGAN -> adding a 1-hot vector encoding of the label to the training data\n",
        "# Simulated Annealing?\n",
        "# Generator -> VAE -> Discriminator?\n",
        "# What about feeding in a dicriminator's confidence level as a temperature during the autoregressive? Inverse confidence?\n",
        "# What about a 3 dimensional GAN?\n",
        "\n",
        "# Umut Notes\n",
        "- Add a stop condition to the softmax\n",
        "- 2 steps process (pick note and then choose how much through binary cross entropy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OeM4sX2NNYT0",
        "outputId": "f965a7cd-c29a-4c73-ecf5-e87bede8dfe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "import mnist\n",
        "import scipy.misc\n",
        "from PIL import Image\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "\n",
        "def create_image(image, name, image_shape=(28, 28)):\n",
        "    img_arr = deepcopy(image.reshape(image_shape)).astype(np.uint8)\n",
        "    # print(\"img shape: {}. img sum: {}\".format(img_arr.shape, img_arr.sum()))\n",
        "    # print(img_arr)\n",
        "    img_arr[img_arr > 0] = 255\n",
        "    # pprint(img_arr)\n",
        "    # print(\"img shape: {}. img sum: {}\".format(img_arr.shape, img_arr.sum()))\n",
        "    img = Image.fromarray(img_arr.astype(np.uint8), 'L')\n",
        "    img.save(name)\n",
        "    return img\n",
        "\n",
        "images = mnist.train_images()\n",
        "num_samples = 10\n",
        "np.random.shuffle(images)\n",
        "images = images[:num_samples, :, :]\n",
        "# For black and white\n",
        "images[images > 0] = 1\n",
        "\n",
        "# For grayscale\n",
        "# images = images / 255.0\n",
        "# images = images.reshape(images.shape[0], -1)\n",
        "pprint(images)\n",
        "print(images.shape)\n",
        "\n",
        "labels = mnist.train_labels()\n",
        "n_labels = np.max(labels) + 1\n",
        "labels = np.eye(n_labels)[labels]\n",
        "print(labels.shape)\n",
        "\n",
        "create_image(images[0], 'my.png')\n",
        "print(labels[0])\n",
        "\n",
        "# images = images[:1000, :, :]\n",
        "# print(images[0].shape)\n",
        "# pprint(images[0])\n",
        "# img = Image.fromarray(images[0], 'L')\n",
        "# img.save('my.png')\n",
        "# img.show()\n",
        "\n",
        "image_shape = np.expand_dims(images[0], axis=-1).shape \n",
        "# print(image_shape)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "array([[[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       ...,\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "       [[0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0],\n",
            "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)\n",
            "(10000, 28, 28)\n",
            "(60000, 10)\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OlmEDomwi9dZ",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model\n",
        "from keras import backend as K\n",
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "def built_in_softmax_kl_loss(target, output):\n",
        "    target = K.flatten(target)\n",
        "    output = K.flatten(output)\n",
        "    \n",
        "    target = target / K.sum(target)\n",
        "    output = K.softmax(output)\n",
        "    return keras.losses.kullback_leibler_divergence(target, output)\n",
        "\n",
        "keras.losses.built_in_softmax_kl_loss = built_in_softmax_kl_loss\n",
        " \n",
        "def unet_model(input_size=(28, 28, 1), n_filters_start=32, growth_factor=2,\n",
        "               upconv=False):\n",
        "    droprate=0.5\n",
        "    n_filters = n_filters_start\n",
        "    inputs = Input(input_size)\n",
        "    conv_first = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv_first = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv_first)\n",
        "    pool_first = MaxPooling2D(pool_size=(2, 2))(conv_first)\n",
        "\n",
        "    prev_pool = pool_first\n",
        "    hidden_layers = []\n",
        "    for _ in range(1):\n",
        "        n_filters *= growth_factor\n",
        "        pool = BatchNormalization()(prev_pool)\n",
        "        conv = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(pool)\n",
        "        conv = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv)\n",
        "        pool = MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "        pool = Dropout(droprate)(pool)\n",
        "        prev_pool = pool\n",
        "        hidden_layers.append(conv)\n",
        " \n",
        "    n_filters *= growth_factor\n",
        "    conv_mid = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(prev_pool)\n",
        "    conv_mid = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv_mid)\n",
        " \n",
        "    n_filters //= growth_factor\n",
        "    if upconv:\n",
        "        up_first = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv_mid), hidden_layers[-1]])\n",
        "    else:\n",
        "        up_first = concatenate([UpSampling2D(size=(2, 2))(conv_mid), hidden_layers[-1]])\n",
        "    up_first = BatchNormalization()(up_first)\n",
        "    conv_mid_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up_first)\n",
        "    conv_mid_2 = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv_mid_2)\n",
        "    conv_mid_2 = Dropout(droprate)(conv_mid_2)\n",
        "\n",
        "    prev_conv = conv_mid_2\n",
        "    for i in range(0):\n",
        "        n_filters //= growth_factor\n",
        "        if upconv:\n",
        "            up = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(prev_conv), hidden_layers[-i-2]])\n",
        "        else:\n",
        "            up = concatenate([UpSampling2D(size=(2, 2))(prev_conv), hidden_layers[-i-2]])\n",
        "        up = BatchNormalization()(up)\n",
        "        conv = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up)\n",
        "        conv = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv)\n",
        "        conv = Dropout(droprate)(conv)\n",
        "        prev_conv = conv\n",
        " \n",
        "    n_filters //= growth_factor\n",
        "    if upconv:\n",
        "        up_last = concatenate([Conv2DTranspose(n_filters, (2, 2), strides=(2, 2), padding='same')(conv_mid_2), conv_first])\n",
        "    else:\n",
        "        up_last = concatenate([UpSampling2D(size=(2, 2))(conv_mid_2), conv_first])\n",
        "    conv_last = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(up_last)\n",
        "    conv_last = Conv2D(n_filters, (3, 3), activation='relu', padding='same')(conv_last)\n",
        " \n",
        "    conv_out = Conv2D(1, 1, activation='linear')(conv_last)\n",
        "    output = Flatten()(conv_out)\n",
        "\n",
        "    flatten = Flatten()(conv_last)\n",
        "    dense = Dense(1, activation='linear')(flatten)\n",
        "\n",
        "    output = concatenate([output, dense])\n",
        " \n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    # model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.compile(optimizer=Adam(), loss=built_in_softmax_kl_loss)\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rSjQr52iQzlu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "17ee9580-4b0d-4d7f-e217-e85992210c65"
      },
      "source": [
        "model = unet_model(input_size=image_shape)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_22\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_24 (InputLayer)           (None, 28, 28, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_254 (Conv2D)             (None, 28, 28, 32)   320         input_24[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_255 (Conv2D)             (None, 28, 28, 32)   9248        conv2d_254[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_47 (MaxPooling2D) (None, 14, 14, 32)   0           conv2d_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 14, 14, 32)   128         max_pooling2d_47[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_256 (Conv2D)             (None, 14, 14, 64)   18496       batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_257 (Conv2D)             (None, 14, 14, 64)   36928       conv2d_256[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_48 (MaxPooling2D) (None, 7, 7, 64)     0           conv2d_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 7, 7, 64)     0           max_pooling2d_48[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_258 (Conv2D)             (None, 7, 7, 128)    73856       dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_259 (Conv2D)             (None, 7, 7, 128)    147584      conv2d_258[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_47 (UpSampling2D) (None, 14, 14, 128)  0           conv2d_259[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 14, 14, 192)  0           up_sampling2d_47[0][0]           \n",
            "                                                                 conv2d_257[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 14, 14, 192)  768         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_260 (Conv2D)             (None, 14, 14, 64)   110656      batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_261 (Conv2D)             (None, 14, 14, 64)   36928       conv2d_260[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 14, 14, 64)   0           conv2d_261[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_48 (UpSampling2D) (None, 28, 28, 64)   0           dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_49 (Concatenate)    (None, 28, 28, 96)   0           up_sampling2d_48[0][0]           \n",
            "                                                                 conv2d_255[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_262 (Conv2D)             (None, 28, 28, 32)   27680       concatenate_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_263 (Conv2D)             (None, 28, 28, 32)   9248        conv2d_262[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_264 (Conv2D)             (None, 28, 28, 1)    33          conv2d_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_16 (Flatten)            (None, 25088)        0           conv2d_263[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "flatten_15 (Flatten)            (None, 784)          0           conv2d_264[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1)            25089       flatten_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_50 (Concatenate)    (None, 785)          0           flatten_15[0][0]                 \n",
            "                                                                 dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 496,962\n",
            "Trainable params: 496,514\n",
            "Non-trainable params: 448\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsFM5XMYeqBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBEdqEBxFqNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# discriminator_model = discriminator(input_size=image_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SVUVU8Kt_aCm",
        "colab": {}
      },
      "source": [
        "from copy import deepcopy\n",
        "import math\n",
        "import itertools\n",
        "def mask_image(image):\n",
        "    image = deepcopy(image)\n",
        "    sampling_percentage = np.random.uniform(0, 100)\n",
        "    non_zero = np.nonzero(image)\n",
        "    mask1 = np.full(len(non_zero[0]), False)\n",
        "    mask1[:math.floor(len(non_zero[0]) * (sampling_percentage/100))] = True\n",
        "    np.random.shuffle(mask1)\n",
        "    # pprint(mask1)\n",
        "    r1 = list(itertools.compress(non_zero[0], mask1))\n",
        "    c1 = list(itertools.compress(non_zero[1], mask1))\n",
        "    output_image = deepcopy(image)\n",
        "    output_image[r1,c1] = 0\n",
        "    return output_image\n",
        "\n",
        "def mask_image_with_noise(image):\n",
        "    image = deepcopy(image)\n",
        "    sampling_percentage_mask = np.random.uniform(0, 100)\n",
        "    sampling_percentage_noise = np.random.uniform(0, 20)\n",
        "    non_zero = np.nonzero(image)\n",
        "    zeroes = np.nonzero(image == 0)\n",
        "    mask = np.full(len(non_zero[0]), False)\n",
        "    noise = np.full(len(zeroes[0]), False) \n",
        "    amount_to_mask = math.floor(len(non_zero[0]) * (sampling_percentage_mask/100))\n",
        "    mask[:amount_to_mask] = True\n",
        "    amount_to_mask_2 = math.floor(len(zeroes[0]) * (sampling_percentage_noise/100))\n",
        "    noise[:amount_to_mask_2] = True\n",
        "    np.random.shuffle(mask)\n",
        "    np.random.shuffle(noise)\n",
        "    # pprint(mask1)\n",
        "    output_image = deepcopy(image)\n",
        "    r1 = list(itertools.compress(non_zero[0], mask))\n",
        "    c1 = list(itertools.compress(non_zero[1], mask))\n",
        "    r2 = list(itertools.compress(zeroes[0], noise))\n",
        "    c2 = list(itertools.compress(zeroes[1], noise))\n",
        "    output_image[r1,c1] = 0\n",
        "    output_image[r2,c2] = 1\n",
        "    return output_image\n",
        "\n",
        "def mask_image_with_noise_grayscale(image):\n",
        "    image = deepcopy(image)\n",
        "    sampling_percentage_mask = np.random.uniform(0, 100)\n",
        "    sampling_percentage_noise = np.random.uniform(0, 20)\n",
        "    non_zero = np.nonzero(image)\n",
        "    zeroes = np.nonzero(image == 0)\n",
        "    mask = np.full(len(non_zero[0]), False)\n",
        "    noise = np.full(len(zeroes[0]), False) \n",
        "    amount_to_mask = math.floor(len(non_zero[0]) * (sampling_percentage_mask/100))\n",
        "    mask[:amount_to_mask] = True\n",
        "    amount_to_mask_2 = math.floor(len(zeroes[0]) * (sampling_percentage_noise/100))\n",
        "    noise[:amount_to_mask_2] = True\n",
        "    np.random.shuffle(mask)\n",
        "    np.random.shuffle(noise)\n",
        "    # pprint(mask1)\n",
        "    output_image = deepcopy(image)\n",
        "    r1 = list(itertools.compress(non_zero[0], mask))\n",
        "    c1 = list(itertools.compress(non_zero[1], mask))\n",
        "    r2 = list(itertools.compress(zeroes[0], noise))\n",
        "    c2 = list(itertools.compress(zeroes[1], noise))\n",
        "    output_image[r1,c1] = -1.0\n",
        "    output_image[r2,c2] = 1.0\n",
        "    return output_image\n",
        "\n",
        "class ImageGenerator(keras.utils.Sequence):\n",
        "    def __init__(self, sample_list, image_shape, batch_size, samples_per_data_item, seed=None):\n",
        "        print(\"sample_list: {}\".format(len(sample_list)))\n",
        "        self.sample_list = sample_list\n",
        "        self.image_shape = image_shape\n",
        "        self.batch_size = batch_size\n",
        "        self.samples_per_data_item = samples_per_data_item\n",
        "        # self.training_input = []\n",
        "        # self.training_target = []\n",
        "        # self.training_original = []\n",
        "        self.sample_index = 0\n",
        "        self.seed = seed\n",
        "        # if self.seed is not None:\n",
        "        #     np.random.seed(self.seed)\n",
        "\n",
        "    def generate_training_pairs(self):\n",
        "        '''\n",
        "        Generates Training Pairs till @training_input / @training_target have @batch_size files.\n",
        "        '''\n",
        "        training_input = []\n",
        "        training_original = []\n",
        "        training_target = []\n",
        "        while len(training_input) < self.batch_size:\n",
        "            original_image = deepcopy(self.sample_list[self.sample_index])\n",
        "            original_image = original_image.reshape(self.image_shape)\n",
        "            original_image[original_image > 0] = 1\n",
        "            self.sample_index = (self.sample_index + 1) % len(self.sample_list)\n",
        "            # print(\"sample_list length: {}. sample_index: {}\".format(\n",
        "            #     len(self.sample_list), self.sample_index))\n",
        "            try:\n",
        "                # augment by adding and removing random values in the array\n",
        "\n",
        "                # Add random values\n",
        "                for _ in range(self.samples_per_data_item):\n",
        "                    input_image = mask_image_with_noise(original_image)\n",
        "\n",
        "                    # xor_target = original_image\n",
        "                    xor_target = np.logical_xor(input_image, original_image)\n",
        "                    input_image = input_image.astype(np.float32)\n",
        "                    xor_target = xor_target.astype(np.float32)\n",
        "                    xor_target = xor_target.flatten()\n",
        "                    xor_target = np.append(xor_target, 0.0)\n",
        "                    original_image = original_image.astype(np.float32)\n",
        "                    training_input.append(deepcopy(input_image))\n",
        "                    training_original.append(deepcopy(original_image))\n",
        "                    training_target.append(deepcopy(xor_target))\n",
        "\n",
        "                # Add original\n",
        "                training_input.append(deepcopy(input_image))\n",
        "                training_original.append(deepcopy(original_image))\n",
        "                training_target.append(deepcopy(xor_target))\n",
        "                \n",
        "            except Exception as e:\n",
        "                print('Error generating input and target pair')\n",
        "                traceback.print_exc()\n",
        "        return np.asarray(training_input), np.asarray(training_target), np.asarray(training_original)\n",
        "\n",
        "    def save_image(self, img_arr, img_name):\n",
        "        img_arr = img_arr.reshape(self.image_shape)\n",
        "        print(img_name)\n",
        "        print(\"img shape: {}. img sum: {}\".format(img_arr.shape, img_arr.sum()))\n",
        "        img_arr = img_arr[:, :, 0]\n",
        "        print(\"img shape: {}. img sum: {}\".format(img_arr.shape, img_arr.sum()))\n",
        "        #pprint(img_arr)\n",
        "        img_arr[img_arr != 0] = 255\n",
        "        #pprint(img_arr)\n",
        "        print(\"img shape: {}. img sum: {}\".format(img_arr.shape, img_arr.sum()))\n",
        "        img = Image.fromarray(img_arr.astype(np.uint8), 'L')\n",
        "        img.save(img_name)\n",
        "\n",
        "    def get_random_training_pair(self):\n",
        "        import random\n",
        "        training_input, training_target, training_original = self.generate_training_pairs()\n",
        "        print(\"training_input shape: {}\".format(training_input.shape))\n",
        "        index = random.randrange(0, len(training_input))\n",
        "        self.save_image(deepcopy(training_input[index]), 'training_input.png')\n",
        "        training_image = training_target[index][:np.prod(self.image_shape)]\n",
        "        print(training_target[index][-1])\n",
        "        self.save_image(deepcopy(training_image), 'training_target.png')\n",
        "        self.save_image(deepcopy(training_original[index]), 'training_original.png')\n",
        "\n",
        "    def generate_validation_samples(self):\n",
        "        old_batch_size = self.batch_size\n",
        "        self.batch_size = len(self.sample_list) * self.samples_per_data_item\n",
        "        training_input, training_target, training_original = self.generate_training_pairs()\n",
        "        # training_input = np.asarray(self.training_input[:self.batch_size])\n",
        "        # training_target = np.asarray(self.training_target[:self.batch_size])\n",
        "        self.batch_size = old_batch_size\n",
        "        return training_input, training_target\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''Generates 1 batch of data'''\n",
        "        training_input, training_target, training_original = self.generate_training_pairs()\n",
        "        # training_input = np.asarray(self.training_input[:self.batch_size])\n",
        "        # training_target = np.asarray(self.training_target[:self.batch_size])\n",
        "        # self.training_input = self.training_input[self.batch_size:]\n",
        "        # self.training_target = self.training_target[self.batch_size:]\n",
        "        # print(\"training input sum: {}. target sum: {}\".format(training_input.sum(), training_target.sum()))\n",
        "        return np.asarray(training_input), np.asarray(training_target)\n",
        "\n",
        "    def __len__(self):\n",
        "        '''Number of batches / epoch'''\n",
        "        # print(\"sample_list: {}. samples_per_data_item: {}, batch size: {}\".\n",
        "        #       format(len(self.sample_list), self.samples_per_data_item,\n",
        "        #              self.batch_size))\n",
        "        samples_to_generate = int(\n",
        "            (len(self.sample_list) * self.samples_per_data_item) /\n",
        "            self.batch_size)\n",
        "        # print(\"samples to generate: {}\".format(samples_to_generate))\n",
        "        return samples_to_generate\n",
        "    \n",
        "    #def on_epoch_end(self):\n",
        "    #    if self.seed is not None:\n",
        "    #        np.random.seed(self.seed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1zwLYew1a-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Config\n",
        "batch_size = 128\n",
        "samples_per_data_item = 1 # add oen to this for the number of TRUTH samples\n",
        "split = 0.8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ngo7o_rw2TsK",
        "outputId": "d59d0976-6dbe-4e8a-b889-e808bb423610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "\n",
        "training_samples = images[:int(len(images) * split)]\n",
        "validation_samples = images[int(len(images) * split):]\n",
        "\n",
        "print(\"training samples: {}. validation samples: {}\".format(len(training_samples), len(validation_samples)))\n",
        "\n",
        "steps_per_epoch = int(len(training_samples) * samples_per_data_item / batch_size)\n",
        "\n",
        "# pprint(training_samples[0])\n",
        "\n",
        "training_generator = ImageGenerator(\n",
        "    sample_list=training_samples,\n",
        "    image_shape=image_shape,\n",
        "    batch_size=batch_size,\n",
        "    samples_per_data_item=samples_per_data_item)\n",
        "\n",
        "validation_generator = ImageGenerator(\n",
        "    sample_list=validation_samples,\n",
        "    image_shape=image_shape,\n",
        "    batch_size=batch_size,\n",
        "    samples_per_data_item=samples_per_data_item)\n",
        "\n",
        "validation_data = validation_generator.generate_validation_samples()\n",
        "\n",
        "# print(\"validation data input and target shape: {}\".format(validation_data[0].shape))\n",
        "training_generator.get_random_training_pair()\n",
        "\n",
        "\n"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training samples: 8000. validation samples: 2000\n",
            "sample_list: 8000\n",
            "sample_list: 2000\n",
            "training_input shape: (128, 28, 28, 1)\n",
            "training_input.png\n",
            "img shape: (28, 28, 1). img sum: 101.0\n",
            "img shape: (28, 28). img sum: 101.0\n",
            "img shape: (28, 28). img sum: 25755.0\n",
            "0.0\n",
            "training_target.png\n",
            "img shape: (28, 28, 1). img sum: 250.0\n",
            "img shape: (28, 28). img sum: 250.0\n",
            "img shape: (28, 28). img sum: 63750.0\n",
            "training_original.png\n",
            "img shape: (28, 28, 1). img sum: 175.0\n",
            "img shape: (28, 28). img sum: 175.0\n",
            "img shape: (28, 28). img sum: 44625.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk-zvNP9DGWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_location = F'/content/drive/My Drive/sc-model.hdf5'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dQKfsm7sEpV0",
        "outputId": "f32d59bc-98c6-4168-e239-edab5c12fce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "\n",
        "model = unet_model(input_size=(28, 28, 1))\n",
        "# model = keras.models.load_model(model_location)\n",
        "\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=model_location,\n",
        "    monitor='val_loss',\n",
        "    save_weights_only=False,\n",
        "    verbose=1,\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir='./logs', update_freq='epoch')\n",
        "\n",
        "if True:\n",
        "    history = model.fit(\n",
        "        training_generator,\n",
        "        validation_data=validation_data,\n",
        "        verbose=1,\n",
        "        shuffle=True,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        epochs=20,\n",
        "        callbacks=[model_checkpoint_callback, tensorboard_callback])\n",
        "    #epochs=cfg.epochs,\n",
        "    #callbacks=callbacks)\n",
        "# model.save(\"sc-model.hdf5\")"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_21\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_22 (InputLayer)           (None, 28, 28, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_232 (Conv2D)             (None, 28, 28, 32)   320         input_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_233 (Conv2D)             (None, 28, 28, 32)   9248        conv2d_232[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_43 (MaxPooling2D) (None, 14, 14, 32)   0           conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 14, 14, 32)   128         max_pooling2d_43[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_234 (Conv2D)             (None, 14, 14, 64)   18496       batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_235 (Conv2D)             (None, 14, 14, 64)   36928       conv2d_234[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_44 (MaxPooling2D) (None, 7, 7, 64)     0           conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 7, 7, 64)     0           max_pooling2d_44[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_236 (Conv2D)             (None, 7, 7, 128)    73856       dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_237 (Conv2D)             (None, 7, 7, 128)    147584      conv2d_236[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_43 (UpSampling2D) (None, 14, 14, 128)  0           conv2d_237[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 14, 14, 192)  0           up_sampling2d_43[0][0]           \n",
            "                                                                 conv2d_235[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 14, 14, 192)  768         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_238 (Conv2D)             (None, 14, 14, 64)   110656      batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_239 (Conv2D)             (None, 14, 14, 64)   36928       conv2d_238[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 14, 14, 64)   0           conv2d_239[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_44 (UpSampling2D) (None, 28, 28, 64)   0           dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 28, 28, 96)   0           up_sampling2d_44[0][0]           \n",
            "                                                                 conv2d_233[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_240 (Conv2D)             (None, 28, 28, 32)   27680       concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_241 (Conv2D)             (None, 28, 28, 32)   9248        conv2d_240[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_242 (Conv2D)             (None, 28, 28, 1)    33          conv2d_241[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 471,873\n",
            "Trainable params: 471,425\n",
            "Non-trainable params: 448\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            " 9/62 [===>..........................] - ETA: 8s - loss: 1.5587 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.135485). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "62/62 [==============================] - 4s 68ms/step - loss: 1.1069 - val_loss: 1.0712\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.07118, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 2/20\n",
            "62/62 [==============================] - 3s 51ms/step - loss: 0.6250 - val_loss: 0.7150\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.07118 to 0.71504, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 3/20\n",
            "62/62 [==============================] - 3s 52ms/step - loss: 0.5390 - val_loss: 0.6029\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.71504 to 0.60287, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 4/20\n",
            "62/62 [==============================] - 3s 52ms/step - loss: 0.5180 - val_loss: 0.5433\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.60287 to 0.54334, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 5/20\n",
            "62/62 [==============================] - 3s 51ms/step - loss: 0.5052 - val_loss: 0.5129\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.54334 to 0.51295, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 6/20\n",
            "62/62 [==============================] - 3s 52ms/step - loss: 0.5007 - val_loss: 0.4997\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.51295 to 0.49966, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 7/20\n",
            "62/62 [==============================] - 3s 52ms/step - loss: 0.4941 - val_loss: 0.4872\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.49966 to 0.48721, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 8/20\n",
            "62/62 [==============================] - 3s 52ms/step - loss: 0.4878 - val_loss: 0.4860\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.48721 to 0.48598, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 9/20\n",
            "62/62 [==============================] - 3s 52ms/step - loss: 0.4891 - val_loss: 0.4818\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.48598 to 0.48175, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 10/20\n",
            "62/62 [==============================] - 3s 51ms/step - loss: 0.4864 - val_loss: 0.4862\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.48175\n",
            "Epoch 11/20\n",
            "62/62 [==============================] - 3s 53ms/step - loss: 0.4797 - val_loss: 0.4801\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.48175 to 0.48006, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 12/20\n",
            "62/62 [==============================] - 3s 53ms/step - loss: 0.4786 - val_loss: 0.4783\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.48006 to 0.47835, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 13/20\n",
            "62/62 [==============================] - 3s 53ms/step - loss: 0.4775 - val_loss: 0.4761\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.47835 to 0.47605, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 14/20\n",
            "62/62 [==============================] - 3s 52ms/step - loss: 0.4750 - val_loss: 0.4705\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.47605 to 0.47052, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 15/20\n",
            "62/62 [==============================] - 3s 53ms/step - loss: 0.4708 - val_loss: 0.4691\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.47052 to 0.46912, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 16/20\n",
            "62/62 [==============================] - 3s 52ms/step - loss: 0.4699 - val_loss: 0.4659\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.46912 to 0.46588, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 17/20\n",
            "62/62 [==============================] - 3s 52ms/step - loss: 0.4687 - val_loss: 0.4635\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.46588 to 0.46348, saving model to /content/drive/My Drive/sc-model.hdf5\n",
            "Epoch 18/20\n",
            "62/62 [==============================] - 3s 52ms/step - loss: 0.4672 - val_loss: 0.4656\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.46348\n",
            "Epoch 19/20\n",
            "62/62 [==============================] - 3s 53ms/step - loss: 0.4639 - val_loss: 0.4646\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.46348\n",
            "Epoch 20/20\n",
            "62/62 [==============================] - 3s 52ms/step - loss: 0.4683 - val_loss: 0.4599\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.46348 to 0.45991, saving model to /content/drive/My Drive/sc-model.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKxrjiG_eqBO",
        "colab_type": "code",
        "outputId": "5b88c650-7577-494e-b52d-7c713a39c653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "# \"Loss\"\n",
        "if True:\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0IEPAcide5lS"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PBu4a18br7wL",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "def inference(model, input_image, directory, iterations, temp_start=2, temp_end=0.5):\n",
        "    create_image(input_image, \"{}/input.png\".format(directory))\n",
        "\n",
        "    temperatures = np.linspace(temp_end, temp_start, num=iterations)[::-1]\n",
        "    # pprint(temperatures)\n",
        "    \n",
        "    working_image = deepcopy(input_image)\n",
        "    working_images = []\n",
        "    num_added = 0\n",
        "    num_removed = 0\n",
        "    for i in range(iterations):\n",
        "        temp = temperatures[i]\n",
        "        predictions = model.predict(working_image)\n",
        "        predictions = predictions.flatten()\n",
        "        predictions = np.exp(predictions / temp)\n",
        "        predictions = predictions / np.sum(predictions)\n",
        "        # print(predictions)\n",
        "        indices = np.arange(predictions.shape[0])\n",
        "        index = np.random.choice(indices, p=predictions)\n",
        "        working_image = working_image.flatten()\n",
        "        if working_image[index] == 1:\n",
        "            num_removed += 1\n",
        "            working_image[index] = 0\n",
        "        elif working_image[index] == 0:\n",
        "            num_added += 1\n",
        "            working_image[index] = 1\n",
        "        else:\n",
        "            print(working_image[index])\n",
        "        working_image = np.reshape(working_image, [1, *image_shape])\n",
        "        if i % 50 == 0:\n",
        "            create_image(working_image, os.path.join(directory, \"working_{}.png\".format(i)))\n",
        "\n",
        "    print(working_image.shape)\n",
        "    print(\"num added: {}. num removed: {}\".format(num_added, num_removed))\n",
        "    img = create_image(working_image, os.path.join(directory, \"final.png\"))\n",
        "    return img, deepcopy(working_image)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJjq3kNqJCOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC6ZVFAFeqBb",
        "colab_type": "code",
        "outputId": "79c035fa-ae77-4b19-9469-f410e41ea5b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "generated_images = []\n",
        "\n",
        "# model = keras.models.load_model(model_location)\n",
        "\n",
        "\n",
        "def generate_noise():\n",
        "    input_image = np.full((28, 28), 0)\n",
        "    # input_image = np.random.rand(28, 28)\n",
        "    # input_image[input_image >= 0.5] = 1\n",
        "    # input_image[input_image < 0.5] = 0\n",
        "    input_image = input_image.astype(np.float32)\n",
        "    input_image = np.expand_dims(input_image, 0)\n",
        "    input_image = np.expand_dims(input_image, -1)\n",
        "    return input_image\n",
        "\n",
        "sample_sqrt = 2\n",
        "for i in range(sample_sqrt**2):\n",
        "    directory = \"images_{}\".format(i)\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    input_image = generate_noise()\n",
        "\n",
        "    img, _ = inference(model, input_image, directory, 200, temp_start=1, temp_end=1)\n",
        "    generated_images.append(img)\n",
        "    \n",
        "final_im = Image.new('RGB', (28 * sample_sqrt, 28 * sample_sqrt))\n",
        "\n",
        "y_offset = 0\n",
        "for i in range(sample_sqrt):\n",
        "    x_offset = 0\n",
        "    new_im = Image.new('RGB', (28 * sample_sqrt, 28))\n",
        "    for j in range(sample_sqrt):\n",
        "        im = generated_images[(i * sample_sqrt) + j]\n",
        "        new_im.paste(im, (x_offset, 0))\n",
        "        x_offset += 28\n",
        "    final_im.paste(new_im, (0, y_offset))\n",
        "    y_offset += 28\n",
        "    \n",
        "final_im.save('final.png')"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 28, 28, 1)\n",
            "num added: 146. num removed: 54\n",
            "(1, 28, 28, 1)\n",
            "num added: 144. num removed: 56\n",
            "(1, 28, 28, 1)\n",
            "num added: 162. num removed: 38\n",
            "(1, 28, 28, 1)\n",
            "num added: 151. num removed: 49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DF783hh7cg_P"
      },
      "source": [
        "## Train Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW-M3GAmeqBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterations = 500\n",
        " \n",
        "generated_samples = []\n",
        "for i in range(int(len(training_samples) / 2)):\n",
        "    directory = \"discriminator\"\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    input_image = generate_noise()\n",
        "\n",
        "    _, generated = inference(model, input_image, directory, iterations, temp_start=1, temp_end=1)\n",
        "    generated_samples.append(generated)\n",
        "\n",
        "print(len(generated_samples))\n",
        "original_gen_samples = deepcopy(generated_samples)\n",
        "generated_samples = np.asarray(generated_samples)\n",
        "print(generated_samples.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2tqKYqmJ3uJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_samples = np.random.rand(int(len(training_samples) / 2), *image_shape)\n",
        "random_samples[random_samples >= 0.5] = 1\n",
        "random_samples[random_samples < 0.5] = 0\n",
        "\n",
        "generated_samples = generated_samples.reshape(-1, *image_shape)\n",
        "images = training_samples\n",
        "images = np.expand_dims(images, axis=-1)\n",
        "\n",
        "discriminator_train_x = np.concatenate((generated_samples, random_samples, images), axis=0)\n",
        "print(generated_samples.shape)\n",
        "print(random_samples.shape)\n",
        "print(images.shape)\n",
        "discriminator_train_y = np.concatenate((np.full((generated_samples.shape[0], 1), 0), \n",
        "                                        np.full((random_samples.shape[0], 1), 0),\n",
        "                                        np.full((images.shape[0], 1), 1)), \n",
        "                                        axis=0)\n",
        "\n",
        "print(discriminator_train_x.shape)\n",
        "print(discriminator_train_y.shape)\n",
        "\n",
        "\n",
        "p = np.random.permutation(len(discriminator_train_x))\n",
        "discriminator_train_x, discriminator_train_y = discriminator_train_x[p], discriminator_train_y[p]\n",
        "print(discriminator_train_x.shape)\n",
        "print(discriminator_train_y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PVue6Y9M0Ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Flatten, Dense, Reshape\n",
        "\n",
        "def discriminator(input_size=(28, 28, 1), n_filters_start=16, growth_factor=2, num_layers=1):\n",
        "    inputs = Input(input_size)\n",
        "    droprate = 0.5\n",
        "    n_filters = n_filters_start\n",
        "    prev_layer = inputs\n",
        "    for _ in range(num_layers):\n",
        "        batch_norm = BatchNormalization()(prev_layer)\n",
        "        conv = Conv2D(n_filters, kernel_size=(3, 3), strides=(2,2), activation='relu', padding='same')(batch_norm)\n",
        "        drop_layer = Dropout(droprate)(conv)\n",
        "        prev_layer = drop_layer\n",
        "        n_filters *= growth_factor\n",
        "\n",
        "    flatten = Flatten()(prev_layer)\n",
        "    # reshape_layer = Reshape((-1, 512))(prev_layer)\n",
        "    validity = Dense(1, activation='sigmoid')(flatten)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=validity)\n",
        "    model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNM3mo1NJeub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator_model = discriminator(input_size=image_shape)\n",
        "model_location = F'/content/drive/My Drive/sc-discrim-model.hdf5'\n",
        "\n",
        "# model = keras.models.load_model(model_location)\n",
        "\n",
        "discrim_batch_size = 64\n",
        "\n",
        "steps_per_epoch = int(len(discriminator_train_x) * split / discrim_batch_size)  \n",
        "validation_steps = int(len(discriminator_train_x) * (1 - split) / discrim_batch_size)  \n",
        "\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=model_location,\n",
        "    monitor='val_loss',\n",
        "    save_weights_only=False,\n",
        "    verbose=1,\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir='./logs', update_freq='epoch')\n",
        "\n",
        "if True:\n",
        "    discrim_history = discriminator_model.fit(\n",
        "        x=discriminator_train_x,\n",
        "        y=discriminator_train_y,\n",
        "        batch_size=discrim_batch_size,\n",
        "        validation_split=split,\n",
        "        verbose=1,\n",
        "        shuffle=True,\n",
        "        epochs=100,\n",
        "        callbacks=[model_checkpoint_callback, tensorboard_callback])\n",
        "    #epochs=cfg.epochs,\n",
        "    #callbacks=callbacks)\n",
        "# model.save(\"sc-model.hdf5\")\n",
        "if True:\n",
        "    plt.plot(discrim_history.history['loss'])\n",
        "    plt.plot(discrim_history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'validation'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz7XvYS3pduE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference_with_discriminator(model, discrim_model, input_image, directory, iterations, temp_low=0.5, temp_high=2):\n",
        "    create_image(input_image, \"{}/input.png\".format(directory))\n",
        "\n",
        "    # pprint(temperatures)\n",
        "    \n",
        "    working_image = deepcopy(input_image)\n",
        "    consecutive = 0\n",
        "    for i in range(iterations):\n",
        "        discrim_predict = discrim_model.predict(working_image)[0]\n",
        "        if discrim_predict >= 0.95:\n",
        "            consecutive += 1\n",
        "            if consecutive >= 100:\n",
        "                break\n",
        "        else:\n",
        "            consecutive = 0\n",
        "\n",
        "        temp = temp_low + ((temp_high - temp_low) * (1 - discrim_predict))\n",
        "        \n",
        "        print(\"temp: {}\".format(temp))\n",
        "        predictions = model.predict(working_image)\n",
        "        predictions = predictions.flatten()\n",
        "        predictions = np.exp(predictions / temp)\n",
        "        predictions = predictions / np.sum(predictions)\n",
        "        # print(predictions)\n",
        "        indices = np.arange(predictions.shape[0])\n",
        "        index = np.random.choice(indices, p=predictions)\n",
        "        working_image = working_image.flatten()\n",
        "        if working_image[index] == 1:\n",
        "            working_image[index] = 0\n",
        "        elif working_image[index] == 0:\n",
        "            working_image[index] = 1\n",
        "        else:\n",
        "            print(working_image[index])\n",
        "        working_image = np.reshape(working_image, [1, *image_shape])\n",
        "        if i % 100 == 0:\n",
        "            create_image(working_image, os.path.join(directory, \"working_{}.png\".format(i)))\n",
        "\n",
        "    print(working_image.shape)\n",
        "    img = create_image(working_image, os.path.join(directory, \"final.png\"))\n",
        "    return img, deepcopy(working_image)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NR7vdWxukd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated_images = []\n",
        "\n",
        "sample_sqrt = 1\n",
        "for i in range(sample_sqrt**2):\n",
        "    directory = \"images_discrim_{}\".format(i)\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    # input_image = images[0]\n",
        "    # input_image = np.expand_dims(input_image, 0)\n",
        "\n",
        " \n",
        "    img, _ = inference_with_discriminator(model, discriminator_model, input_image, directory, 10000)\n",
        "    generated_images.append(img)\n",
        "    \n",
        "final_im = Image.new('RGB', (28 * sample_sqrt, 28 * sample_sqrt))\n",
        "\n",
        "y_offset = 0\n",
        "for i in range(sample_sqrt):\n",
        "    x_offset = 0\n",
        "    new_im = Image.new('RGB', (28 * sample_sqrt, 28))\n",
        "    for j in range(sample_sqrt):\n",
        "        im = generated_images[(i * sample_sqrt) + j]\n",
        "        new_im.paste(im, (x_offset, 0))\n",
        "        x_offset += 28\n",
        "    final_im.paste(new_im, (0, y_offset))\n",
        "    y_offset += 28\n",
        "    \n",
        "final_im.save('final_with_discrim.png')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}